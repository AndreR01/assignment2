#!#!/usr/bin/env bash
# Author: Andre Rampono
# Student: 23840638
set -x
# Data cleaning script to take three arguments, clean the data and send resulting data to STDOUT
# check the files are tab seperated
# report any lines that do not have the same number of cells to stderr
# clean the data - date ranges, code column is empty
# determine which files contain which data (homicides will only have NF = 4)

# sort files according to entity (column 1)
# create and store key:val array for entity and year
# create a tsv file containing the header list outlined in asssignment
# use the key:val (entityYear) array to reference the rows in the cleaned files and build the new data file
# remove temp files

# Check CLI for valid number of arguments
if [[ $# -ne 3 ]]
then
    echo "Usage: $0 <tsv datafile> <tsv datafile> <tsv datafile>." > /dev/stderr
    exit 1
fi

# Check file for validity
for arg in "$@"
do
    if [[ ! -s $arg ]]
    then
        echo "Error: The file $arg does not exist or is not readable." > /dev/stderr
        exit 1
    fi
done

# TODO Choose tsv checker

# Check header line for a tab
for arg in "$@"
do
    if head -1 $arg | grep -q ' '
    then
        echo tsv
    else
        echo not a tsv
    fi
done

#TODO Implement a loop to compare the current line NF to the head NF and alert if different
# Check each line against header tabs
for arg in "$@"
do
    num_tab=$(head -n1 $arg | awk '{print gsub(/\t/,"")}')
    num_col=$(head -n1 $arg | awk -F'\t' '{print NF;exit}')

    echo $num_col
    awk -F'\t' '{print NF}' $arg
    awk -v cols=$num_col '{if (NF != cols) {printf"Line %d missing cells \n ", NR}}' $arg
done

#TODO Add back code to check the tab separated heading and check each line for same number of cells

# Process each input file
for arg in "$@"; do
    header=$(head -1 "$arg")
    echo "$header" > header_extract.tsv
    # - check the header for life, gdp and homicide
    if echo "$header" | grep -E -q [Ll]ife; then
    # - ignore rows outside date range and ignore rows without a country code
    # - sort by entity and delete continent column from life and gdp
        awk -F'\t' 'BEGIN {OFS=FS} (int($3) < 2011 || int($3) > 2021) || $2 == "" {next} {print $0}' "$arg" | sort -k1,1 -k3,3n | cut -f1-6 > _life.tsv
    # - concat entity and year column to create unique column key
        awk 'BEGIN {FS="\t"; OFS=FS} {$7=$2$3}1' _life.tsv | sort -k7 > _tmplife.tsv
    elif echo "$header" | grep -E -q [Gg][Dd][Pp]; then
    # - ignore rows outside date range and ignore rows without a country code
    # - sort by entity and delete continent column from life and gdp
        awk -F'\t' 'BEGIN {OFS=FS} (int($3) < 2011 || int($3) > 2021) || $2 == "" {next} {print $0}' "$arg" | sort -k1,1 -k3,3n | cut -f1-6 > _gdp.tsv
        awk 'BEGIN {FS="\t"; OFS=FS} {$7=$2$3}1' _gdp.tsv | sort -k7 > _tmpgdp.tsv
    elif echo "$header" | grep -q [Hh]omicide; then
    # - ignore rows outside date range and ignore rows without a country code
        awk -F'\t' 'BEGIN {OFS=FS} (int($3) < 2011 || int($3) > 2021) || $2 == "" {next} {print $0}' "$arg" | sort -k1,1 -k3,3n | cut -f1-4 > _homicide.tsv
    # - concat entity and year column to create unique column key
        awk 'BEGIN {FS="\t"; OFS=FS} {print $5=$2$3}' _homicide.tsv | sort -k5 > _tmphomicide.tsv
    fi
done
#TODO remove header_extract.tsv
#TODO remove temp files

#join -1 7 -2 7 -o 1.1 1.2 1.3 1.4 1.5 1.6 2.5 2.6 _tmplife.tsv _tmpgdp.tsv > merged1.tsv
#join -1 7 -2 5 -o 1.1 1.2 1.3 1.4 1.5 1.6 2.4 _tmplife.tsv _tmphomicide.tsv > merged2.tsv
#join -1 7 -2 7 -o 1.1 1.2 1.3 1.7 1.8 2.7 1.5 1.6 merged1.tsv merged2.tsv > merged.tsv

#join -1 7 -2 7 -o 1.1 1.2 1.3 1.4 1.5 1.6 2.1 2.2 2.3 2.4 2.5 2.6 _tmplife.tsv _tmpgdp.tsv > merged1.tsv 
#join -1 7 -2 5 -o 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 1.10 1.11 1.12 2.1 2.2 2.3 2.4 _tmplife.tsv _tmphomicide.tsv > merged2.tsv 
#join -1 1 -2 1 -o 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 1.10 1.11 1.12 2.1 2.2 2.3 2.4 merged1.tsv merged2.tsv > merged.tsv








